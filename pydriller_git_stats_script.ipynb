{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pydriller import Repository\n",
    "from pydriller.metrics.process.lines_count import LinesCount\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import lxml.html as lx\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def pull_request_frequency(repo_link):\n",
    "    username, repo_name = repo_link.split('/')[-2:]\n",
    "    \n",
    "    open_prs_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=open'\n",
    "    open_prs_response = requests.get(open_prs_url)\n",
    "    open_prs_count = len(open_prs_response.json())\n",
    "    \n",
    "    merged_prs_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=closed&sort=updated&direction=desc'\n",
    "    merged_prs_response = requests.get(merged_prs_url)\n",
    "    merged_prs_count = len(merged_prs_response.json())\n",
    "    \n",
    "    return open_prs_count, merged_prs_count\n",
    "\n",
    "def fetch_forks_stars(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html = lx.fromstring(response.text)\n",
    "    fork_element = html.xpath('//*[@id=\"repo-network-counter\"]')[0]\n",
    "    star_element = html.xpath('//*[@id=\"repo-stars-counter-star\"]')[0]\n",
    "    forks = int(fork_element.text)\n",
    "    stars = int(star_element.text)\n",
    "    return forks, stars\n",
    "\n",
    "def issues_pending(git_repo_url):\n",
    "    result=requests.get(git_repo_url)\n",
    "    html = lx.fromstring(result.text)\n",
    "    issues_tab = html.xpath('//a[@id=\"issues-tab\"]')\n",
    "    if issues_tab:\n",
    "        issues_url = git_repo_url + \"/issues\"\n",
    "        result=requests.get(issues_url)\n",
    "        html = lx.fromstring(result.text)\n",
    "        open_issues = html.xpath('//a[@data-ga-click=\"Issues, Table state, Open\"]')\n",
    "        if open_issues:\n",
    "            open_issues_content = open_issues[0].text_content()\n",
    "            open_issues_count = int(open_issues_content.split()[0].replace(',', ''))\n",
    "            return open_issues_count\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def issues_resolved(git_repo_url):\n",
    "    result=requests.get(git_repo_url)\n",
    "    html = lx.fromstring(result.text)\n",
    "    issues_tab = html.xpath('//a[@id=\"issues-tab\"]')\n",
    "    if issues_tab:\n",
    "        issues_url = git_repo_url + \"/issues\"\n",
    "        result=requests.get(issues_url)\n",
    "        html = lx.fromstring(result.text)\n",
    "        closed_issues = html.xpath('//a[@data-ga-click=\"Issues, Table state, Closed\"]')\n",
    "        if closed_issues:\n",
    "            closed_issues_content = closed_issues[0].text_content()\n",
    "            closed_issues_count = int(closed_issues_content.split()[0].replace(',', ''))\n",
    "            return closed_issues_count\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# def make_github_request(url):\n",
    "#     while True:\n",
    "#         token = \"\"  # Add your GitHub personal access token here\n",
    "#         headers = {'Authorization': f'Bearer {token}'}\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         if response.status_code == 200:\n",
    "#             return response\n",
    "#         elif response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n",
    "#             sleep_time = get_rate_limit_reset_time(response) + 5  # Adding 5 seconds to be safe\n",
    "#             print(f\"Rate limit exceeded. Waiting for {sleep_time} seconds before retrying.\")\n",
    "#             time.sleep(sleep_time)\n",
    "#         else:\n",
    "#             response.raise_for_status()\n",
    "\n",
    "# def get_rate_limit_reset_time(response):\n",
    "#     rate_limit_reset = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "#     return max(rate_limit_reset - time.time(), 0)\n",
    "\n",
    "def pr_freq(repo_link, total_days):\n",
    "    username, repo_name = repo_link.split('/')[-2:]\n",
    "\n",
    "    pr_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=all'\n",
    "    response = requests.get(pr_url)\n",
    "    all_prs = response.json()\n",
    "\n",
    "    total_pull_requests = len(all_prs)\n",
    "    pull_request_frequency = total_pull_requests / total_days\n",
    "\n",
    "    return pull_request_frequency\n",
    "\n",
    "def release_frequency(github_repo_url):\n",
    "    release_dates = []\n",
    "    for commit in Repository(github_repo_url, only_releases=True).traverse_commits():\n",
    "        release_dates.append(commit.committer_date)\n",
    "    \n",
    "    if(len(release_dates)>1):\n",
    "        # Calculate the difference between the last and first release dates\n",
    "        difference = release_dates[-1] - release_dates[0]\n",
    "        \n",
    "        # Get the number of days from the timedelta object\n",
    "        number_of_days = difference.days\n",
    "        number_of_releases = len(release_dates)\n",
    "    \n",
    "        release_frequency_days = number_of_releases / number_of_days\n",
    "    \n",
    "        # Convert release frequency to months and years\n",
    "        release_frequency_months = number_of_releases / (number_of_days / 30)  # Assuming 30 days in a month\n",
    "        release_frequency_years = number_of_releases / (number_of_days / 365)  # Assuming 365 days in a year\n",
    "\n",
    "    else:\n",
    "        return 0,0,0\n",
    "\n",
    "    return release_frequency_days, release_frequency_months, release_frequency_years\n",
    "\n",
    "\n",
    "def get_loc(github_repo_url):\n",
    "    total_added = 0\n",
    "    total_deleted = 0\n",
    "    \n",
    "    commit_dates = []\n",
    "    for commit in Repository(github_repo_url).traverse_commits():\n",
    "        commit_dates.append(commit.committer_date)\n",
    "        \n",
    "    if(len(commit_dates)!=0):\n",
    "        first_commit_date = commit_dates[0]\n",
    "        last_commit_date = commit_dates[-1]\n",
    "    \n",
    "        # Initialize LinesCount\n",
    "        lines_count = LinesCount(path_to_repo=github_repo_url,since=first_commit_date,to=last_commit_date)\n",
    "        \n",
    "        total_added = lines_count.count_added()\n",
    "        total_deleted = lines_count.count_removed()\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "    LOC = {}\n",
    "    for key in total_added:\n",
    "        if key in total_deleted:\n",
    "            LOC[key] = total_added[key] - total_deleted[key]\n",
    "        else:\n",
    "            LOC[key] = total_added[key]\n",
    "\n",
    "    Filtered_LOC = {}\n",
    "    \n",
    "    for file in LOC:\n",
    "        if(file[-3:] == \".js\"):\n",
    "            if(\"test\" not in file and \"tests\" not in file and \".spec.js\" not in file and \".test.js\" not in file and \".spec.ts\" not in file and \".test.ts\" not in file and \"spec\" not in file):\n",
    "                if(\"node_modules\\\\\" not in file and \"public\\\\\" not in file and \"build\\\\\" not in file and \"test\\\\\" not in file):\n",
    "                    Filtered_LOC[file] = LOC[file]\n",
    "    return sum(Filtered_LOC.values())\n",
    "\n",
    "\n",
    "def get_total_lines_added_deleted(github_repo_url):\n",
    "    total_added = 0\n",
    "    total_deleted = 0\n",
    "    \n",
    "    commit_dates = []\n",
    "    for commit in Repository(github_repo_url).traverse_commits():\n",
    "        commit_dates.append(commit.committer_date)\n",
    "        \n",
    "    if(len(commit_dates)!=0):\n",
    "        first_commit_date = commit_dates[0]\n",
    "        last_commit_date = commit_dates[-1]\n",
    "    \n",
    "        # Initialize LinesCount\n",
    "        lines_count = LinesCount(path_to_repo=github_repo_url,since=first_commit_date,to=last_commit_date)\n",
    "        \n",
    "        total_added = lines_count.count_added()\n",
    "    \n",
    "        total_deleted = lines_count.count_removed()\n",
    "\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "    return sum(total_added.values()), sum(total_deleted.values())\n",
    "\n",
    "\n",
    "def get_lines_added_deleted_last_one_and_half_years(github_repo_url):\n",
    "    total_added = 0\n",
    "    total_deleted = 0\n",
    "    current_date = datetime.now()\n",
    "    one_and_a_half_years_ago = current_date - timedelta(days=547)  # Assuming 1 year = 365 days so 547 for 1.5 years\n",
    "    year = one_and_a_half_years_ago.year\n",
    "    month = one_and_a_half_years_ago.month\n",
    "    day = one_and_a_half_years_ago.day\n",
    "    one_and_a_half_years_ago_date = datetime(year, month, day)\n",
    "    lines_count = LinesCount(path_to_repo=github_repo_url, since=one_and_a_half_years_ago_date, to=current_date)\n",
    "    total_added = lines_count.count_added()\n",
    "    total_deleted = lines_count.count_removed()\n",
    "    return sum(total_added.values()), sum(total_deleted.values())\n",
    "\n",
    "def is_readme_updated_in_last_one_and_half_years(github_repo_url):\n",
    "    total_added = 0\n",
    "    total_deleted = 0\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # Calculate the date 1.5 years ago\n",
    "    one_and_a_half_years_ago = current_date - timedelta(days=547)  # Assuming 1 year = 365 days\n",
    "    \n",
    "    # Extract year, month, and day components\n",
    "    year = one_and_a_half_years_ago.year\n",
    "    month = one_and_a_half_years_ago.month\n",
    "    day = one_and_a_half_years_ago.day\n",
    "    \n",
    "    one_and_a_half_years_ago_date = datetime(year, month, day)\n",
    "    \n",
    "    # Initialize LinesCount\n",
    "    lines_count = LinesCount(path_to_repo=github_repo_url, since=one_and_a_half_years_ago_date, to=current_date)\n",
    "    \n",
    "    total_added = lines_count.count_added()\n",
    "\n",
    "    total_deleted = lines_count.count_removed()\n",
    "    \n",
    "    lowercase_total_added = {key.lower() if key is not None else key: value for key, value in total_added.items()}\n",
    "    lowercase_total_deleted = {key.lower() if key is not None else key: value for key, value in total_deleted.items()}\n",
    "\n",
    "    try:\n",
    "        readme_added = lowercase_total_added['readme.md']\n",
    "        readme_deleted = lowercase_total_deleted['readme.md']\n",
    "        if(readme_added+readme_deleted==0):\n",
    "            readme_updated = False\n",
    "        else:\n",
    "            readme_updated = True\n",
    "    except:\n",
    "        readme_updated = False\n",
    "\n",
    "    return readme_updated\n",
    "\n",
    "def get_github_repo_sloc(github_url):\n",
    "    repo_name = \"temp_repo\"\n",
    "    try:\n",
    "        subprocess.run(['git', 'clone', github_url, repo_name], check=True)\n",
    "\n",
    "        result = subprocess.run(['npx', 'cloc', repo_name, '--json', '--exclude-dir=node_modules,public,build, test', '--not-match-f=test|tests|\\\\.spec\\\\.js|\\\\.test\\\\.js|\\\\.spec\\\\.ts|\\\\.test\\\\.ts|spec'], capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(\"Error occurred while running cloc command:\")\n",
    "            print(result.stderr)\n",
    "            return 0\n",
    "        \n",
    "        loc_data = json.loads(result.stdout)\n",
    "        \n",
    "        if 'JavaScript' in loc_data:\n",
    "            return loc_data['JavaScript']\n",
    "        else:\n",
    "            return 0\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 0\n",
    "    finally:\n",
    "        subprocess.run(['rm', '-rf', repo_name])\n",
    "\n",
    "\n",
    "def analyze_repository(url):\n",
    "    total_commits = 0\n",
    "    commit_dates = []\n",
    "    contributors = set()\n",
    "\n",
    "    try:\n",
    "        forks, stars = fetch_forks_stars(url)\n",
    "        open_issues = issues_pending(url)\n",
    "        resolved_issues = issues_resolved(url)\n",
    "        total_commits, avg_commits_per_day, last_commit_date, unique_contributors = 0, 0, None, 0\n",
    "\n",
    "        repo = Repository(url)\n",
    "        for commit in repo.traverse_commits():\n",
    "            total_commits += 1\n",
    "            commit_dates.append(commit.committer_date)\n",
    "            contributors.add(commit.author.email)\n",
    "\n",
    "        if total_commits > 0:\n",
    "            first_commit_date = min(commit_dates)\n",
    "            last_commit_date = max(commit_dates)\n",
    "            unique_contributors = len(contributors)\n",
    "            total_days = (last_commit_date - first_commit_date).days + 1\n",
    "            avg_commits_per_day = total_commits / total_days if total_days > 0 else 0\n",
    "\n",
    "        open_prs_count, merged_prs_count = pull_request_frequency(url)\n",
    "        pr_frequency = pr_freq(url, total_days)\n",
    "        pr_frequency = round(pr_frequency, 2)\n",
    "        version_release_frequency_days, version_release_frequency_months, version_release_frequency_years = release_frequency(url)\n",
    "        lines_of_codes = get_loc(url)\n",
    "        # source_lines_of_code = get_github_repo_sloc(url)\n",
    "        source_lines_of_code = None\n",
    "        total_added, total_deleted = get_total_lines_added_deleted(url)\n",
    "        lines_added_one_and_half_year,lines_deleted_one_and_half_year = get_lines_added_deleted_last_one_and_half_years(url)\n",
    "        readme_updated = is_readme_updated_in_last_one_and_half_years(url)\n",
    "        \n",
    "\n",
    "        return total_commits, avg_commits_per_day, last_commit_date, unique_contributors, forks, stars, open_prs_count, merged_prs_count, open_issues, resolved_issues, pr_frequency, version_release_frequency_days, version_release_frequency_months, version_release_frequency_years,lines_of_codes, source_lines_of_code, total_added, total_deleted, lines_added_one_and_half_year, lines_deleted_one_and_half_year, readme_updated\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing repository {url}: {e}\")\n",
    "        return None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "def fetch_and_analyze_repositories(input_csv, output_csv):\n",
    "    unique_repos = set()\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            repo_url = row.get('latest_version_git_repo')\n",
    "            if repo_url and repo_url not in unique_repos:\n",
    "                print(f\"Analyzing repository: {repo_url}\")\n",
    "                total_commits, avg_commits_per_day, last_commit_date, unique_contributors, forks, stars, open_prs_count, merged_prs_count, open_issues, resolved_issues, pr_frequency, version_release_frequency_days, version_release_frequency_months, version_release_frequency_years,lines_of_codes, source_lines_of_code, total_added, total_deleted, lines_added_one_and_half_year, lines_deleted_one_and_half_year, readme_updated = analyze_repository(repo_url)\n",
    "                if total_commits is not None:\n",
    "                    data.append({\n",
    "                        'git_repo': repo_url,\n",
    "                        'total_commits': total_commits,\n",
    "                        'avg_commits_per_day': avg_commits_per_day,\n",
    "                        'last_commit_date': last_commit_date,\n",
    "                        'unique_contributors': unique_contributors,\n",
    "                        'forks': forks,\n",
    "                        'stars': stars,\n",
    "                        'open_PRs': open_prs_count,\n",
    "                        'merged_PRs': merged_prs_count,\n",
    "                        'open_issues': open_issues,\n",
    "                        'resolved_issues': resolved_issues,\n",
    "                        'pr_frequency': pr_frequency,\n",
    "                        'version_release_frequency_days': version_release_frequency_days, 'version_release_frequency_months': version_release_frequency_months, 'version_release_frequency_years': version_release_frequency_years,\n",
    "                        'LOC' : lines_of_codes,\n",
    "                        'SLOC': source_lines_of_code,\n",
    "                        'total_lines_added': total_added,\n",
    "                        'total_lines_deleted': total_deleted,\n",
    "                        'lines_added_one_and_half_year': lines_added_one_and_half_year,\n",
    "                        'lines_deleted_one_and_half_year': lines_deleted_one_and_half_year,\n",
    "                        'readme_updated': readme_updated\n",
    "                    })\n",
    "                unique_repos.add(repo_url)\n",
    "            elif repo_url:\n",
    "                print(f\"Skipping duplicate repository: {repo_url}\")\n",
    "            else:\n",
    "                print(\"Skipping repository: value not present\")\n",
    "\n",
    "    with open(output_csv, 'w', newline='') as file:\n",
    "        fieldnames = ['git_repo', 'total_commits', 'avg_commits_per_day', 'last_commit_date', 'unique_contributors', 'forks', 'stars', 'open_PRs', 'merged_PRs', 'open_issues', 'resolved_issues', 'pr_frequency', 'version_release_frequency_days', 'version_release_frequency_months', 'version_release_frequency_years' 'LOC', 'SLOC', 'total_lines_added', 'total_lines_deleted', 'lines_added_one_and_half_year', 'lines_deleted_one_and_half_year', 'readme_updated']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "input_file = '/kaggle/input/pydriller-test-2/npm_package_data_final.csv'\n",
    "fetch_and_analyze_repositories(input_file, 'git_stats.csv')\n",
    "\n",
    "\n",
    "# SLOC\n",
    "# complexity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
