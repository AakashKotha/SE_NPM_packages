{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pydriller import Repository\n",
    "from pydriller.metrics.process.lines_count import LinesCount\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import lxml.html as lx\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def pull_request_frequency(repo_link):\n",
    "    username, repo_name = repo_link.split('/')[-2:]\n",
    "    \n",
    "    open_prs_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=open'\n",
    "    open_prs_response = requests.get(open_prs_url)\n",
    "    open_prs_count = len(open_prs_response.json())\n",
    "    \n",
    "    merged_prs_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=closed&sort=updated&direction=desc'\n",
    "    merged_prs_response = requests.get(merged_prs_url)\n",
    "    merged_prs_count = len(merged_prs_response.json())\n",
    "    \n",
    "    return open_prs_count, merged_prs_count\n",
    "\n",
    "def fetch_forks_stars(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html = lx.fromstring(response.text)\n",
    "    fork_element = html.xpath('//*[@id=\"repo-network-counter\"]')[0]\n",
    "    star_element = html.xpath('//*[@id=\"repo-stars-counter-star\"]')[0]\n",
    "    forks = int(fork_element.text)\n",
    "    stars = int(star_element.text)\n",
    "    return forks, stars\n",
    "\n",
    "def issues_pending(git_repo_url):\n",
    "    result=requests.get(git_repo_url)\n",
    "    html = lx.fromstring(result.text)\n",
    "    issues_tab = html.xpath('//a[@id=\"issues-tab\"]')\n",
    "    if issues_tab:\n",
    "        issues_url = git_repo_url + \"/issues\"\n",
    "        result=requests.get(issues_url)\n",
    "        html = lx.fromstring(result.text)\n",
    "        open_issues = html.xpath('//a[@data-ga-click=\"Issues, Table state, Open\"]')\n",
    "        if open_issues:\n",
    "            open_issues_content = open_issues[0].text_content()\n",
    "            open_issues_count = int(open_issues_content.split()[0].replace(',', ''))\n",
    "            return open_issues_count\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def issues_resolved(git_repo_url):\n",
    "    result=requests.get(git_repo_url)\n",
    "    html = lx.fromstring(result.text)\n",
    "    issues_tab = html.xpath('//a[@id=\"issues-tab\"]')\n",
    "    if issues_tab:\n",
    "        issues_url = git_repo_url + \"/issues\"\n",
    "        result=requests.get(issues_url)\n",
    "        html = lx.fromstring(result.text)\n",
    "        closed_issues = html.xpath('//a[@data-ga-click=\"Issues, Table state, Closed\"]')\n",
    "        if closed_issues:\n",
    "            closed_issues_content = closed_issues[0].text_content()\n",
    "            closed_issues_count = int(closed_issues_content.split()[0].replace(',', ''))\n",
    "            return closed_issues_count\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# def make_github_request(url):\n",
    "#     while True:\n",
    "#         token = \"\"  # Add your GitHub personal access token here\n",
    "#         headers = {'Authorization': f'Bearer {token}'}\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         if response.status_code == 200:\n",
    "#             return response\n",
    "#         elif response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n",
    "#             sleep_time = get_rate_limit_reset_time(response) + 5  # Adding 5 seconds to be safe\n",
    "#             print(f\"Rate limit exceeded. Waiting for {sleep_time} seconds before retrying.\")\n",
    "#             time.sleep(sleep_time)\n",
    "#         else:\n",
    "#             response.raise_for_status()\n",
    "\n",
    "# def get_rate_limit_reset_time(response):\n",
    "#     rate_limit_reset = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "#     return max(rate_limit_reset - time.time(), 0)\n",
    "\n",
    "def pr_freq(repo_link, total_days):\n",
    "    username, repo_name = repo_link.split('/')[-2:]\n",
    "\n",
    "    pr_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=all'\n",
    "    response = requests.get(pr_url)\n",
    "    all_prs = response.json()\n",
    "\n",
    "    total_pull_requests = len(all_prs)\n",
    "    pull_request_frequency = total_pull_requests / total_days\n",
    "\n",
    "    return pull_request_frequency\n",
    "\n",
    "# def release_history(repo_link):\n",
    "#     username, repo_name = repo_link.split('/')[-2:]\n",
    "\n",
    "#     releases_url = f'https://api.github.com/repos/{username}/{repo_name}/releases'\n",
    "#     response = requests.get(releases_url)\n",
    "#     releases = response.json()\n",
    "    \n",
    "#     if releases:\n",
    "#         # Extracting published dates\n",
    "#         published_dates = [release['published_at'] for release in releases]\n",
    "#         # Sorting dates chronologically\n",
    "#         sorted_dates = sorted(published_dates)\n",
    "#         # Converting dates from string to datetime format\n",
    "#         datetime_dates = [datetime.strptime(date, '%Y-%m-%dT%H:%M:%SZ') for date in sorted_dates]\n",
    "#         # Calculate frequency between the first and last release\n",
    "#         first_release_date = datetime_dates[0]\n",
    "#         last_release_date = datetime_dates[-1]\n",
    "#         total_days = (last_release_date - first_release_date).days + 1\n",
    "#         release_frequency = len(releases) / total_days if total_days > 0 else 0\n",
    "#         return release_frequency.round(release_frequency,2)\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "def get_loc(github_repo_url):\n",
    "    loc = 0\n",
    "    for commit in Repository(github_repo_url).traverse_commits():\n",
    "        loc += commit.lines\n",
    "    return loc\n",
    "\n",
    "def get_total_lines_added_deleted(github_repo_url):\n",
    "    total_added = 0\n",
    "    total_deleted = 0\n",
    "    repo = Repository(github_repo_url)\n",
    "    first_commit = next(repo.traverse_commits(), None)\n",
    "    first_commit_hash = first_commit.hash if first_commit else None\n",
    "    last_commit = next(repo.traverse_commits(), None)\n",
    "    for commit in repo.traverse_commits():\n",
    "        last_commit = commit\n",
    "    last_commit_hash = last_commit.hash\n",
    "    lines_count = LinesCount(path_to_repo=github_repo_url,from_commit=first_commit_hash,to_commit=last_commit_hash)\n",
    "    total_added = lines_count.count_added()\n",
    "    total_deleted = lines_count.count_removed()\n",
    "\n",
    "    return sum(total_added.values()), sum(total_deleted.values())\n",
    "\n",
    "def get_lines_added_deleted_last_one_and_half_years(github_repo_url):\n",
    "    total_added = 0\n",
    "    total_deleted = 0\n",
    "    current_date = datetime.now()\n",
    "    one_and_a_half_years_ago = current_date - timedelta(days=547)  # Assuming 1 year = 365 days so 547 for 1.5 years\n",
    "    year = one_and_a_half_years_ago.year\n",
    "    month = one_and_a_half_years_ago.month\n",
    "    day = one_and_a_half_years_ago.day\n",
    "    one_and_a_half_years_ago_date = datetime(year, month, day)\n",
    "    lines_count = LinesCount(path_to_repo=github_repo_url, since=one_and_a_half_years_ago_date, to=current_date)\n",
    "    total_added = lines_count.count_added()\n",
    "    total_deleted = lines_count.count_removed()\n",
    "    return sum(total_added.values()), sum(total_deleted.values())\n",
    "\n",
    "def is_readme_updated_in_last_one_and_half_years(github_repo_url):\n",
    "    total_added = 0\n",
    "    total_deleted = 0\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # Calculate the date 1.5 years ago\n",
    "    one_and_a_half_years_ago = current_date - timedelta(days=547)  # Assuming 1 year = 365 days\n",
    "    \n",
    "    # Extract year, month, and day components\n",
    "    year = one_and_a_half_years_ago.year\n",
    "    month = one_and_a_half_years_ago.month\n",
    "    day = one_and_a_half_years_ago.day\n",
    "    \n",
    "    one_and_a_half_years_ago_date = datetime(year, month, day)\n",
    "    \n",
    "    # Initialize LinesCount\n",
    "    lines_count = LinesCount(path_to_repo=github_repo_url, since=one_and_a_half_years_ago_date, to=current_date)\n",
    "    \n",
    "    total_added = lines_count.count_added()\n",
    "\n",
    "    total_deleted = lines_count.count_removed()\n",
    "    \n",
    "    lowercase_total_added = {key.lower() if key is not None else key: value for key, value in total_added.items()}\n",
    "    lowercase_total_deleted = {key.lower() if key is not None else key: value for key, value in total_deleted.items()}\n",
    "\n",
    "    try:\n",
    "        readme_added = lowercase_total_added['readme.md']\n",
    "        readme_deleted = lowercase_total_deleted['readme.md']\n",
    "        if(readme_added+readme_deleted==0):\n",
    "            readme_updated = False\n",
    "        else:\n",
    "            readme_updated = True\n",
    "    except:\n",
    "        readme_updated = False\n",
    "\n",
    "    return readme_updated\n",
    "\n",
    "\n",
    "def analyze_repository(url):\n",
    "    total_commits = 0\n",
    "    commit_dates = []\n",
    "    contributors = set()\n",
    "\n",
    "    try:\n",
    "        forks, stars = fetch_forks_stars(url)\n",
    "        open_issues = issues_pending(url)\n",
    "        resolved_issues = issues_resolved(url)\n",
    "        total_commits, avg_commits_per_day, last_commit_date, unique_contributors = 0, 0, None, 0\n",
    "\n",
    "        repo = Repository(url)\n",
    "        for commit in repo.traverse_commits():\n",
    "            total_commits += 1\n",
    "            commit_dates.append(commit.committer_date)\n",
    "            contributors.add(commit.author.email)\n",
    "\n",
    "        if total_commits > 0:\n",
    "            first_commit_date = min(commit_dates)\n",
    "#             avg_commits_per_day = total_commits / ((max(commit_dates) - min(commit_dates)).days + 1)\n",
    "            last_commit_date = max(commit_dates)\n",
    "            unique_contributors = len(contributors)\n",
    "            total_days = (last_commit_date - first_commit_date).days + 1\n",
    "            avg_commits_per_day = total_commits / total_days if total_days > 0 else 0\n",
    "\n",
    "        open_prs_count, merged_prs_count = pull_request_frequency(url)\n",
    "        pr_frequency = pr_freq(url, total_days)\n",
    "        pr_frequency = round(pr_frequency, 2)\n",
    "#         release_frequency = release_history(url)\n",
    "        lines_of_codes = get_loc(url)\n",
    "        total_added, total_deleted = get_total_lines_added_deleted(url)\n",
    "        lines_added_one_and_half_year,lines_deleted_one_and_half_year = get_lines_added_deleted_last_one_and_half_years(url)\n",
    "        readme_updated = is_readme_updated_in_last_one_and_half_years(url)\n",
    "#         total_added, total_deleted = None, None\n",
    "        \n",
    "\n",
    "        return total_commits, avg_commits_per_day, last_commit_date, unique_contributors, forks, stars, open_prs_count, merged_prs_count, open_issues, resolved_issues, pr_frequency, lines_of_codes, total_added, total_deleted, lines_added_one_and_half_year, lines_deleted_one_and_half_year, readme_updated\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing repository {url}: {e}\")\n",
    "        return None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "def fetch_and_analyze_repositories(input_csv, output_csv):\n",
    "    unique_repos = set()\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            repo_url = row.get('latest_version_git_repo')\n",
    "            if repo_url and repo_url not in unique_repos:\n",
    "                print(f\"Analyzing repository: {repo_url}\")\n",
    "                total_commits, avg_commits_per_day, last_commit_date, unique_contributors, forks, stars, open_prs_count, merged_prs_count, open_issues, resolved_issues, pr_frequency, lines_of_codes, total_added, total_deleted, lines_added_one_and_half_year, lines_deleted_one_and_half_year, readme_updated = analyze_repository(repo_url)\n",
    "                if total_commits is not None:\n",
    "                    data.append({\n",
    "                        'git_repo': repo_url,\n",
    "                        'total_commits': total_commits,\n",
    "                        'avg_commits_per_day': avg_commits_per_day,\n",
    "                        'last_commit_date': last_commit_date,\n",
    "                        'unique_contributors': unique_contributors,\n",
    "                        'forks': forks,\n",
    "                        'stars': stars,\n",
    "                        'open_PRs': open_prs_count,\n",
    "                        'merged_PRs': merged_prs_count,\n",
    "                        'open_issues': open_issues,\n",
    "                        'resolved_issues': resolved_issues,\n",
    "                        'pr_frequency': pr_frequency,\n",
    "#                         'release_frequency': release_frequency,\n",
    "                        'LOC' : lines_of_codes,\n",
    "                        'total_lines_added': total_added,\n",
    "                        'total_lines_deleted': total_deleted,\n",
    "                        'lines_added_one_and_half_year': lines_added_one_and_half_year,\n",
    "                        'lines_deleted_one_and_half_year': lines_deleted_one_and_half_year,\n",
    "                        'readme_updated': readme_updated\n",
    "                    })\n",
    "                unique_repos.add(repo_url)\n",
    "            elif repo_url:\n",
    "                print(f\"Skipping duplicate repository: {repo_url}\")\n",
    "            else:\n",
    "                print(\"Skipping repository: value not present\")\n",
    "\n",
    "    with open(output_csv, 'w', newline='') as file:\n",
    "        fieldnames = ['git_repo', 'total_commits', 'avg_commits_per_day', 'last_commit_date', 'unique_contributors', 'forks', 'stars', 'open_PRs', 'merged_PRs', 'open_issues', 'resolved_issues', 'pr_frequency', 'LOC', 'total_lines_added', 'total_lines_deleted', 'lines_added_one_and_half_year', 'lines_deleted_one_and_half_year', 'readme_updated']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "input_file = '/kaggle/input/pydriller-test-2/npm_package_data_final.csv'\n",
    "fetch_and_analyze_repositories(input_file, 'git_stats.csv')\n",
    "\n",
    "\n",
    "# added lines & deleted lines -> done\n",
    "# pr freq -> done \n",
    "# release history -> done\n",
    "# LOC ->done\n",
    "# readme updated -> done"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
